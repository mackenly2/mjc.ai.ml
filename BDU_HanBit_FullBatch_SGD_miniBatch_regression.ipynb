{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNNfXa17ktae4Tn3V/VNEGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjungweapon/mjc.ai.ml/blob/BDU/BDU_HanBit_FullBatch_SGD_miniBatch_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NeMTCMYmbbQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. 데이터 생성\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------------------\n",
        "# 1. Full-batch Gradient Descent (LinearRegression)\n",
        "# ----------------------\n",
        "model_full = LinearRegression()  # 내부적으로 전체 데이터를 사용\n",
        "model_full.fit(X_train, y_train)\n",
        "y_pred_full = model_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# ----------------------\n",
        "# 2. Stochastic Gradient Descent (SGDRegressor, batch_size=1)\n",
        "# ----------------------\n",
        "model_sgd = SGDRegressor(learning_rate='constant', eta0=0.001, max_iter=1, tol=None, random_state=42)\n",
        "\n",
        "# 반복적으로 한 샘플씩 학습\n",
        "for epoch in range(100):\n",
        "    for i in range(len(X_train)):\n",
        "        model_sgd.partial_fit(X_train[i:i+1], y_train[i:i+1])\n",
        "\n",
        "y_pred_sgd = model_sgd.predict(X_test)\n",
        "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
        "\n",
        "# ----------------------\n",
        "# 3. Mini-batch Gradient Descent (SGDRegressor, batch_size=32)\n",
        "# ----------------------\n",
        "model_minibatch = SGDRegressor(learning_rate='constant', eta0=0.001, max_iter=1, tol=None, random_state=42)\n",
        "\n",
        "batch_size = 32\n",
        "n_batches = int(np.ceil(len(X_train) / batch_size))\n",
        "\n",
        "for epoch in range(100):\n",
        "    indices = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        X_batch = X_train_shuffled[start:end]\n",
        "        y_batch = y_train_shuffled[start:end]\n",
        "        model_minibatch.partial_fit(X_batch, y_batch)\n",
        "\n",
        "y_pred_minibatch = model_minibatch.predict(X_test)\n",
        "mse_minibatch = mean_squared_error(y_test, y_pred_minibatch)\n",
        "\n",
        "# ----------------------\n",
        "# 결과 비교\n",
        "# ----------------------\n",
        "print(f\"Full-batch MSE     : {mse_full:.4f}\")\n",
        "print(f\"SGD MSE (batch=1)  : {mse_sgd:.4f}\")\n",
        "print(f\"Mini-batch MSE (32): {mse_minibatch:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vf73fre-nLnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 예측 vs 실제값 시각화 함수\n",
        "def plot_predictions(y_test, preds, labels):\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    for i, (y_pred, label) in enumerate(zip(preds, labels)):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # y=x 선\n",
        "        plt.xlabel(\"True Values\")\n",
        "        plt.ylabel(\"Predictions\")\n",
        "        plt.title(f\"{label} Prediction\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 시각화 실행\n",
        "plot_predictions(\n",
        "    y_test,\n",
        "    [y_pred_full, y_pred_sgd, y_pred_minibatch],\n",
        "    [\"Full-batch\", \"SGD (batch=1)\", \"Mini-batch (32)\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "pn5hsNHRnMMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"예측 결과만 보면 세 방법 모두 학습을 잘 했기 때문에 그래프가 비슷하게 나옵니다.\n",
        "하지만 학습하는 과정에서의 차이는 분명히 존재하고, 특히 데이터가 커지거나 모델이 복잡해질수록\n",
        "'속도', '안정성', '자원 사용량'에서 큰 차이를 보이게 됩니다."
      ],
      "metadata": {
        "id": "elpMviogocUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n",
        "\n",
        "# 1. 데이터 생성\n",
        "X, y = make_regression(n_samples=3000, n_features=20, noise=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 학습 파라미터\n",
        "epochs = 50\n",
        "eta0 = 0.001\n",
        "\n",
        "def run_training(name, batch_size):\n",
        "    model = SGDRegressor(learning_rate='constant', eta0=eta0, max_iter=1, tol=None, random_state=42)\n",
        "    n = len(X_train)\n",
        "    losses = []\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(n)\n",
        "        X_shuffled = X_train[indices]\n",
        "        y_shuffled = y_train[indices]\n",
        "        epoch_loss = 0\n",
        "        count = 0\n",
        "\n",
        "        for i in range(0, n, batch_size):\n",
        "            X_batch = X_shuffled[i:i + batch_size]\n",
        "            y_batch = y_shuffled[i:i + batch_size]\n",
        "            model.partial_fit(X_batch, y_batch)\n",
        "\n",
        "            # 예측 및 손실 계산\n",
        "            y_pred = model.predict(X_batch)\n",
        "            loss = mean_squared_error(y_batch, y_pred)\n",
        "            epoch_loss += loss\n",
        "            count += 1\n",
        "\n",
        "        losses.append(epoch_loss / count)\n",
        "\n",
        "    end = time.time()\n",
        "    total_time = end - start\n",
        "    return losses, total_time\n",
        "\n",
        "# Run Full-batch (batch_size = 전체 데이터 수)\n",
        "loss_full, time_full = run_training(\"Full-batch\", batch_size=len(X_train))\n",
        "\n",
        "# Run SGD (batch_size = 1)\n",
        "loss_sgd, time_sgd = run_training(\"SGD\", batch_size=1)\n",
        "\n",
        "# Run Mini-batch (batch_size = 32)\n",
        "loss_mini, time_mini = run_training(\"Mini-batch\", batch_size=32)\n",
        "\n",
        "# -------------------------------\n",
        "# 📈 그래프 1: Loss vs Epoch\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_full, label=f\"Full-batch ({time_full:.2f}s)\")\n",
        "plt.plot(loss_sgd, label=f\"SGD (batch=1) ({time_sgd:.2f}s)\")\n",
        "plt.plot(loss_mini, label=f\"Mini-batch (batch=32) ({time_mini:.2f}s)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error (Loss)\")\n",
        "plt.title(\"Loss vs Epoch (Training Stability & Speed)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 📋 결과 출력\n",
        "# -------------------------------\n",
        "print(\"총 학습 시간:\")\n",
        "print(f\"Full-batch : {time_full:.2f}초\")\n",
        "print(f\"SGD        : {time_sgd:.2f}초\")\n",
        "print(f\"Mini-batch : {time_mini:.2f}초\")\n"
      ],
      "metadata": {
        "id": "ZzPJ-Skiodlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vbvKJpOyqx6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n",
        "\n",
        "# 1. 데이터 준비\n",
        "X, y = make_regression(n_samples=2000, n_features=10, noise=20, random_state=0)\n",
        "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# 2. 학습 함수\n",
        "def train_model(batch_size, eta0, epochs=50):\n",
        "    model = SGDRegressor(learning_rate='constant', eta0=eta0, max_iter=1, tol=None, random_state=0)\n",
        "    losses = []\n",
        "    n = len(X_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(n)\n",
        "        X_shuffled = X_train[indices]\n",
        "        y_shuffled = y_train[indices]\n",
        "        epoch_loss = 0\n",
        "        count = 0\n",
        "\n",
        "        for i in range(0, n, batch_size):\n",
        "            X_batch = X_shuffled[i:i+batch_size]\n",
        "            y_batch = y_shuffled[i:i+batch_size]\n",
        "            model.partial_fit(X_batch, y_batch)\n",
        "            y_pred = model.predict(X_batch)\n",
        "            loss = mean_squared_error(y_batch, y_pred)\n",
        "            epoch_loss += loss\n",
        "            count += 1\n",
        "        losses.append(epoch_loss / count)\n",
        "    return losses\n",
        "\n",
        "# 3. 실험 조합: (batch_size, eta0)\n",
        "configs = [\n",
        "    (1, 0.0001),   # 작은 배치 + 작은 학습률\n",
        "    (1, 0.01),     # 작은 배치 + 큰 학습률\n",
        "    (32, 0.001),   # 중간 배치 + 중간 학습률\n",
        "    (len(X_train), 0.001),  # Full-batch + 중간 학습률\n",
        "    (len(X_train), 0.05),   # Full-batch + 큰 학습률\n",
        "]\n",
        "\n",
        "# 4. 학습 및 그래프 시각화\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "for batch_size, eta0 in configs:\n",
        "    losses = train_model(batch_size, eta0)\n",
        "    label = f\"Batch={batch_size}, LR={eta0}\"\n",
        "    plt.plot(losses, label=label)\n",
        "\n",
        "plt.title(\"Loss vs Epoch (Learning Rate vs Batch Size)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rf5I5uNsqyW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시각화 해석 가이드\n",
        "진동이 큰 그래프: 작은 배치, 큰 학습률\n",
        "\n",
        "느리게 수렴하는 그래프: 작은 학습률\n",
        "\n",
        "빠르고 안정적으로 수렴: 중간 배치 + 적당한 학습률\n",
        "\n",
        "Full-batch는 매우 부드러운 수렴, 그러나 느릴 수 있음"
      ],
      "metadata": {
        "id": "tvvhzaR3reTU"
      }
    }
  ]
}