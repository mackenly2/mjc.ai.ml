{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNjP4fmpgjb2Rc7e1aNeoUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjungweapon/mjc.ai.ml/blob/BDU/BDU_naive_bayes_exmaple_animation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 **Scikit-learn의 MultinomialNB (Multinomial Naive Bayes)를** 사용해, 스팸 메일 분류와 유사한 아주 쉬운 예제를 보여드릴게요. 텍스트 데이터를 다루며, 파이프라인까지 포함되어 있어 바로 실행할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "kQaw9nlFBq51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "< 접근 방법 >\n",
        "\n",
        "주요 포인트\n",
        "CountVectorizer()가 텍스트를 단어 빈도 벡터로 변환합니다.\n",
        "\n",
        "MultinomialNB()는 단어 등장 횟수를 기반으로 학습합니다.\n",
        "\n",
        "make_pipeline은 벡터화 → 분류기를 한 번에 묶어줍니다.\n",
        "\n",
        "결과는 confusion matrix와 precision, recall, f1-score로 평가됩니다."
      ],
      "metadata": {
        "id": "g9EbwIuuG6U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. import 하기"
      ],
      "metadata": {
        "id": "PshG9nuKCq4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "Az-fs-7wCuPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. data 모으기"
      ],
      "metadata": {
        "id": "7o37opk1CyaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 샘플 데이터\n",
        "texts = [\n",
        "    \"Win money now!\", \"Claim your prize\", \"Hello friend\", \"Let's have lunch\",\n",
        "    \"Congratulations, you have won!\", \"Are we still meeting tomorrow?\",\n",
        "    \"This is not spam\", \"Free entry in 2 a weekly competition to win!\",\n",
        "    \"Call me later\", \"Urgent! You have won a 1 week free membership\"\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1, 0, 0, 1, 0, 1]  # 1: spam, 0: not spam"
      ],
      "metadata": {
        "id": "_BgZxLxyC0dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. data 분리"
      ],
      "metadata": {
        "id": "YDBWlMV2C6oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "VeW5Ev8qC8o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. pipe line  만들기"
      ],
      "metadata": {
        "id": "6fg1FGLJDBCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 파이프라인: 텍스트 벡터화 + Naive Bayes 분류\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())"
      ],
      "metadata": {
        "id": "pln4dPFUDEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make_pipeline 함수를 나이브 베이즈 모델 적용 시 사용하셨군요! 여기서 pipeline이라는 것은 데이터 전처리 단계와 모델 학습 단계를 순차적으로 연결하여 하나의 흐름으로 관리하는 도구를 의미합니다. 마치 공장의 조립 라인처럼, 데이터가 파이프라인을 따라 흐르면서 여러 단계를 거쳐 최종 결과(예측)를 만들어내는 과정을 나타냅니다.\n",
        "\n",
        "sklearn.pipeline.Pipeline의 역할:\n",
        "\n",
        "make_pipeline 함수나 Pipeline 클래스를 사용하는 주요 목적은 다음과 같습니다.\n",
        "\n",
        "편리한 모델 구축: 여러 단계의 작업을 하나의 객체로 묶어 코드의 가독성을 높이고 모델 구축 과정을 단순화합니다. 예를 들어, 특성 스케일링, 차원 축소, 모델 학습 등의 단계를 순서대로 정의하고 한 번의 fit, predict 호출로 전체 과정을 실행할 수 있습니다.\n",
        "\n",
        "교차 검증의 용이성: 파이프라인을 사용하면 교차 검증 시 각 폴드마다 동일한 전처리 단계를 적용할 수 있습니다. 이는 데이터 누출(data leakage)을 방지하여 모델의 일반화 성능을 더 정확하게 평가하는 데 도움을 줍니다. 만약 파이프라인 없이 교차 검증을 수행한다면, 각 폴드마다 전처리 단계를 반복해야 하고 실수할 가능성도 높아집니다.\n",
        "\n",
        "모델 배포 및 관리의 용이성: 학습된 파이프라인 객체를 저장하고 불러와서 새로운 데이터에 동일한 전처리 과정을 거친 후 예측을 수행할 수 있습니다. 이는 모델 배포 및 관리 측면에서 매우 효율적입니다.\n",
        "\n",
        "나이브 베이즈 모델 적용 시 파이프라인의 예시:\n",
        "\n",
        "나이브 베이즈 모델을 적용할 때, 종종 다음과 같은 전처리 단계를 함께 사용합니다.\n",
        "\n",
        "텍스트 데이터의 경우:\n",
        "CountVectorizer 또는 TfidfVectorizer: 텍스트 데이터를 단어 빈도 또는 TF-IDF 값으로 변환합니다.\n",
        "Feature Scaling (드물지만): 경우에 따라 특성 스케일링이 필요할 수 있습니다.\n",
        "수치형 데이터의 경우:\n",
        "StandardScaler 또는 MinMaxScaler: 특성들의 스케일을 조정하여 모델 학습의 안정성을 높입니다.\n",
        "PCA (Principal Component Analysis): 차원을 축소하여 모델의 복잡성을 줄이고 성능을 향상시킬 수 있습니다.\n",
        "이러한 전처리 단계와 나이브 베이즈 모델을 파이프라인으로 묶으면 다음과 같은 형태가 될 수 있습니다.\n",
        "\n",
        "Python\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# 텍스트 데이터에 Multinomial Naive Bayes 모델 적용\n",
        "text_pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# 수치형 데이터에 Gaussian Naive Bayes 모델 적용 (스케일링 포함)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "numeric_pipeline = make_pipeline(StandardScaler(), GaussianNB())\n",
        "위의 예시에서 text_pipeline은 텍스트 데이터를 TF-IDF로 변환한 후 Multinomial Naive Bayes 모델로 학습하는 과정을 하나의 객체로 묶었습니다. 마찬가지로 numeric_pipeline은 수치형 데이터를 표준 스케일링한 후 Gaussian Naive Bayes 모델로 학습하는 과정을 묶었습니다.\n",
        "\n",
        "이제 text_pipeline이나 numeric_pipeline 객체에 대해 fit 메서드를 호출하면, 파이프라인 내의 모든 단계가 순서대로 실행됩니다. predict 메서드를 호출하면, 새로운 데이터에 대해 동일한 전처리 과정을 거친 후 나이브 베이즈 모델이 예측 결과를 반환합니다.\n",
        "\n",
        "결론적으로, make_pipeline을 사용하여 나이브 베이즈 모델을 적용할 때, 파이프라인은 데이터 전처리부터 모델 학습까지의 일련의 과정을 효율적으로 관리하고 실행할 수 있도록 도와주는 매우 유용한 도구입니다."
      ],
      "metadata": {
        "id": "d05Ke38DE8s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 학습 하기"
      ],
      "metadata": {
        "id": "iAr3Xr8GDutL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "lUMWspJ6DxgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 예측 하기"
      ],
      "metadata": {
        "id": "g-gZficeD04O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. 예측\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "zOkyHsmVD2S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 평가 하기"
      ],
      "metadata": {
        "id": "FjZtjNCND5J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. 평가\n",
        "print(\"=== 예측 결과 ===\")\n",
        "print(y_pred)\n",
        "print(\"\\n=== 정확도 평가 ===\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n=== 분류 리포트 ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Not Spam\", \"Spam\"]))\n"
      ],
      "metadata": {
        "id": "1_kQBjWVD7Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "animation\n"
      ],
      "metadata": {
        "id": "2Bw8yPbiCV13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 1. 샘플 텍스트 & 레이블\n",
        "texts = [\n",
        "    \"Win money now!\", \"Claim your prize\", \"Hello friend\", \"Let's have lunch\",\n",
        "    \"Congratulations, you have won!\", \"Are we still meeting tomorrow?\",\n",
        "    \"This is not spam\", \"Free entry in 2 a weekly competition to win!\",\n",
        "    \"Call me later\", \"Urgent! You have won a 1 week free membership\"\n",
        "]\n",
        "labels = [1,1,0,0,1,0,0,1,0,1]  # 1: spam, 0: not spam\n",
        "\n",
        "# 2. train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# 3. 벡터화 & PCA (시각화용)\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train).toarray()   # <-- toarray() 추가\n",
        "texts_vec    = vectorizer.transform(texts).toarray()\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "texts_pca = pca.fit_transform(texts_vec)\n",
        "\n",
        "# 4. partial-fit 가능한 NB 모델 준비\n",
        "partial_model = MultinomialNB()\n",
        "n_train = len(X_train_vec)\n",
        "\n",
        "# 5. 애니메이션 준비\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "colors = ['blue','red']  # 0:not spam, 1:spam\n",
        "\n",
        "# 모델을 fit으로 초기화\n",
        "partial_model.fit(X_train_vec, y_train)\n",
        "\n",
        "def init():\n",
        "    ax.clear()\n",
        "    ax.set_xlim(texts_pca[:,0].min()-1, texts_pca[:,0].max()+1)\n",
        "    ax.set_ylim(texts_pca[:,1].min()-1, texts_pca[:,1].max()+1)\n",
        "    ax.set_title(\"Naive Bayes Learning Progress\")\n",
        "    ax.grid(True)\n",
        "    return []\n",
        "\n",
        "def update(frame):\n",
        "    ax.clear()\n",
        "    # 0프레임: 미학습 상태\n",
        "    if frame > 0:\n",
        "        Xi = X_train_vec[frame-1:frame]\n",
        "        yi = [y_train[frame-1]]\n",
        "        # 첫 호출에만 classes 인자 전달\n",
        "        partial_model.partial_fit(Xi, yi, classes=np.array([0,1]))\n",
        "    # 현재 모델로 전체 텍스트 예측\n",
        "    preds = partial_model.predict(texts_vec)\n",
        "    # 시각화\n",
        "    for i, (x,y) in enumerate(texts_pca):\n",
        "        ax.scatter(x, y,\n",
        "                   c=colors[preds[i]],\n",
        "                   edgecolor='k',\n",
        "                   s=100,\n",
        "                   alpha=0.6)\n",
        "        ax.text(x+0.02, y+0.02, str(i), fontsize=8)\n",
        "    ax.set_title(f\"Step {frame}/{n_train}\")\n",
        "    ax.set_xlim(texts_pca[:,0].min()-1, texts_pca[:,0].max()+1)\n",
        "    ax.set_ylim(texts_pca[:,1].min()-1, texts_pca[:,1].max()+1)\n",
        "    ax.grid(True)\n",
        "    return []\n",
        "\n",
        "# 6. 애니메이션 실행\n",
        "ani = FuncAnimation(\n",
        "    fig, update, frames=n_train+1,\n",
        "    init_func=init, interval=800, repeat=False\n",
        ")\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "btmL06NwCXSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WK0dxsFxEW1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab에서 그대로 복붙하세요.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 1) 2D 합성 데이터 생성\n",
        "np.random.seed(0)\n",
        "X, y = make_classification(\n",
        "    n_samples=200, n_features=2,\n",
        "    n_redundant=0, n_informative=2,\n",
        "    n_clusters_per_class=1, random_state=42\n",
        ")\n",
        "\n",
        "# 2) 학습 순서 정하기 (여기선 앞 50개만 사용)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "indices = indices[:50]\n",
        "n_steps = len(indices)\n",
        "\n",
        "# 3) Gaussian Naive Bayes 모델 준비\n",
        "model = GaussianNB()\n",
        "\n",
        "# 4) 결정 경계 그릴 메쉬 그리드 준비\n",
        "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 100),\n",
        "    np.linspace(y_min, y_max, 100)\n",
        ")\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# 5) 플롯 세팅\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "\n",
        "def update(step):\n",
        "    ax.clear()\n",
        "    idx = indices[step]\n",
        "    xi, yi = X[idx:idx+1], y[idx:idx+1]\n",
        "\n",
        "    # 첫 step 에만 classes 지정\n",
        "    if step == 0:\n",
        "        model.partial_fit(xi, yi, classes=np.unique(y))\n",
        "    else:\n",
        "        model.partial_fit(xi, yi)\n",
        "\n",
        "    # 경계 그리기\n",
        "    Z = model.predict(grid).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "\n",
        "    # 원본 데이터 전체를 현재 예측 색으로 표시\n",
        "    preds = model.predict(X)\n",
        "    ax.scatter(X[:,0], X[:,1], c=preds, cmap='RdBu', edgecolor='k')\n",
        "\n",
        "    ax.set_title(f'Gaussian NB Learning — Step {step+1}/{n_steps}')\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.grid(True)\n",
        "\n",
        "# 6) 애니메이션 생성 & Colab에서 HTML 로 표시\n",
        "ani = FuncAnimation(fig, update, frames=n_steps, interval=500, repeat=False)\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "8XGiSWB_FoRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive baysian 에 대해서  구체적인 예로 다시 설명해 보자\n",
        "\n",
        "가상의 예시: 날씨에 따른 활동 분류\n",
        "\n",
        "우리가 \"맑음\", \"흐림\", \"비\" 세 가지 날씨 조건에 따라 사람들이 \"공원\", \"실내\", \"집\" 중 어떤 활동을 하는지 분류하는 간단한 예시를 생각해 보자.\n",
        "\n",
        "우리의 목적 ??  **날씨 조건에 따라 어떤 활동을 할 것인지를 예측**"
      ],
      "metadata": {
        "id": "f7XulpFbI_la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "날씨,활동\n",
        "맑음,공원\n",
        "맑음,공원\n",
        "흐림,실내\n",
        "비,집\n",
        "비,집\n",
        "맑음,공원\n",
        "흐림,실내\n",
        "흐림,집\n",
        "비,집\n",
        "맑음,공원\n",
        "```\n"
      ],
      "metadata": {
        "id": "Rt6OJUIHJZy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 정보로 부터,  필요한 확률을 계산해 보자 (손으로)\n",
        "\n",
        "1. 사전 확률 (prior Probability )\n",
        "  \n",
        "     P(공원) = 4 / 10 = 0.4\n",
        "     P(집)   = 4 / 10 = 0.4\n",
        "     P(실내) = 2/ 10 = 0.2\n",
        "\n",
        "2. 각 날씨 조건 하에서의 활동 ( Likelihood ,우도) 확룔\n",
        "   \n",
        "     P(맑음|공원) = 4/4 = 1.0  ( 공원 활동 중, 날씨 맑은 조건에서 )\n",
        "     P(흐림|공원) = 0 / 4 = 0.0\n",
        "     P(비|공원) =  0 / 4 = 0.0\n",
        "\n",
        "     P(맑음|실내)   0 / 2 = 0.0\n",
        "     P(흐림|실내)   2 / 2 = 1\n",
        "     P(비|실내)     0 / 2 = 0.0\n",
        "\n",
        "     P(맑음|집)     0 / 4\n",
        "     P(흐림|집)     1 / 4   = 0.25\n",
        "     P(비|집)       3/ 4    = 0.75\n",
        "\n",
        "3. 이제 사후 확률 계산하자. P(H|D) = P(H) x P(D|H)  / P(D)  // H는 가설, D는 data\n",
        "\n",
        "   새로운 날씨 조건이 '흐림' 으로 주어 졌을 때의 사후  확률 은 ( P(D)는 동일 조건이라 생략, 어차피 크기(확률) 비교 이므로 )\n",
        "\n",
        "   P(공원) x P(흐림|공원)   = 0.4 x 0.0 = 0.0\n",
        "   P(집) X P(흐림|집)     =  0.4 x 0.25 = 0.1\n",
        "   P(실내) x P(흐림|실내) =   0.2 x 1.0 = 0.2\n",
        "\n",
        "\n",
        "#   ==> 결과 ,  위 내용을 보면  사후 확률을 비교해 보면, \"흐림\" 날씨에는 \"실내\" 활동을 할 확률이 가장 높습니다. 따라서 나이브 베이즈 모델은 \"흐림\"이라는 입력에 대해 \"실내\"라고 예측할 것\n"
      ],
      "metadata": {
        "id": "kK2pfP-7JkNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1) 데이터 준비\n",
        "data = {\n",
        "    '날씨': ['맑음','맑음','흐림','비','비','맑음','흐림','흐림','비','맑음'],\n",
        "    '활동': ['공원','공원','실내','집','집','공원','실내','집','집','공원']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2) 인코딩\n",
        "le_weather  = LabelEncoder().fit(df['날씨'])\n",
        "le_activity = LabelEncoder().fit(df['활동'])\n",
        "\n",
        "X = le_weather.transform(df['날씨']).reshape(-1,1)\n",
        "y = le_activity.transform(df['활동'])\n",
        "\n",
        "# 3) 모델 학습\n",
        "model = CategoricalNB()   ### 카테고리 분포(categorical distribution) 를 따른다고 가정\n",
        "model.fit(X, y)\n",
        "\n",
        "# 4) 사전확률 출력 (로그→지수화)\n",
        "log_priors = model.class_log_prior_\n",
        "priors     = np.exp(log_priors)\n",
        "print(\"클래스별 사전확률 (prior):\")\n",
        "for cls_idx, prop in enumerate(priors):\n",
        "    print(f\"  {le_activity.inverse_transform([cls_idx])[0]}: {prop:.2f}\")\n",
        "\n",
        "# 또는 class_count_ 사용\n",
        "counts = model.class_count_\n",
        "print(\"\\nclass_count_ 기반 사전확률:\")\n",
        "for cls_idx, cnt in enumerate(counts):\n",
        "    print(f\"  {le_activity.inverse_transform([cls_idx])[0]}: {cnt/counts.sum():.2f}\")\n",
        "\n",
        "# 5) 조건부 확률 출력\n",
        "print(\"\\n날씨별 활동 조건부 확률 P(활동|날씨):\")\n",
        "# feature_log_prob_[0]은 첫 번째(그리고 유일한) 특성에 대한 로그우도 배열:\n",
        "# shape = (n_classes, n_categories_of_날씨)\n",
        "log_likelihood = model.feature_log_prob_[0]\n",
        "for cat_idx, weather in enumerate(le_weather.classes_):\n",
        "    print(f\"\\n날씨 = {weather}\")\n",
        "    for cls_idx, activity in enumerate(le_activity.classes_):\n",
        "        prob = np.exp(log_likelihood[cls_idx, cat_idx])\n",
        "        print(f\"  P(활동={le_activity.inverse_transform([cls_idx])[0]} | 날씨) = {prob:.2f}\")\n"
      ],
      "metadata": {
        "id": "rPPH4oNXJcAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 설명:\n",
        "\n",
        "데이터 준비: 판다스 DataFrame을 이용하여 학습 데이터를 생성합니다.\n",
        "\n",
        "특성과 타겟 분리: '날씨'를 특성으로, '활동'을 타겟으로 분리합니다.\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Label Encoding: 나이브 베이즈 모델은 일반적으로 숫자형 입력을 받기 때문에, LabelEncoder를 사용하여 범주형 데이터를 숫자로 변환합니다. \"공원\" -> 0, \"실내\" -> 1, \"집\" -> 2 와 같이 인코딩됩니다. \"맑음\" -> 0, \"비\" -> 1, \"흐림\" -> 2 와 같이 날씨도 인코딩됩니다.\n",
        "\n",
        "\n",
        "모델 초기화 및 학습: CategoricalNB 모델을 초기화하고\n",
        "학습 데이터로 fit 메서드를 호출하여 모델을 학습시킵니다.\n",
        "\n",
        "(*) CategoricalNB는 범주형 특성을 다루는 나이브 베이즈 모델입니다.\n",
        "새로운 데이터 예측: 새로운 날씨 \"흐림\"을 인코딩하여 모델의 predict 메서드에 입력하고, 예측된 활동을 다시 디코딩하여 사람이 읽을 수 있는 형태로 출력합니다.\n",
        "\n",
        "\n",
        "확률 확인:\n",
        "model.class_prior_: 학습된 각 클래스의 사전 확률을 보여줍니다. (예: [0.4, 0.2, 0.4] -> 공원, 실내, 집 순서)\n",
        "model.feature_log_prob_: 각 클래스 내에서 각 특성(날씨)의 조건부 확률의 로그 값을 보여줍니다. 로그 값을 사용하면 곱셈을 덧셈으로 처리하여 계산의 안정성을 높일 수 있습니다.\n",
        "model.category_count_[i] / model.class_count_[i]: 각 클래스 내에서 특정 날씨가 나타난 횟수를 해당 클래스의 총 데이터 수로 나누어 실제 조건부 확률을 계산합니다.\n",
        "결과 분석:\n",
        "\n",
        "코드 실행 결과에서 \"흐림\" 날씨에 대해 \"실내\" 활동이 예측된 것을 확인할 수 있습니다. model.class_prior_를 통해 각 활동의 사전 확률을, model.feature_log_prob_와 실제 비율 계산을 통해 각 날씨 조건 하에서 특정 활동이 나타날 조건부 확률(우도)을 확인할 수 있습니다.\n",
        "\n",
        "이 코드를 통해 나이브 베이즈 알고리즘이 학습 데이터에서 확률 정보를 학습하고, 새로운 데이터에 대해 이 확률 정보를 이용하여 예측을 수행하는 기본적인 과정을 이해하는 데 도움이 되셨기를 바랍니다."
      ],
      "metadata": {
        "id": "sTKWiPxXObI_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k36NRbTcOcUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial NB 적용의 예. ( sklearn의 20 newsgroup )"
      ],
      "metadata": {
        "id": "ran0i9CiVX9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Google Colab에서 matplotlib 애니메이션을 보여주기 위한 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 한글 표시를 위한 설정\n",
        "!pip install koreanize-matplotlib\n",
        "import koreanize_matplotlib\n",
        "\n",
        "\n",
        "\n",
        "# 1. 데이터 로드\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']  # 일부 카테고리만 선택\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. 파이프라인 구성 (벡터화 + Naive Bayes)\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),         # 단어 개수 기반 벡터화\n",
        "    ('tfidf', TfidfTransformer()),       # TF-IDF 변환\n",
        "    ('clf', MultinomialNB()),            # Naive Bayes 분류기\n",
        "])\n",
        "\n",
        "# 3. 모델 학습\n",
        "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "# 4. 예측 및 평가\n",
        "predicted = text_clf.predict(newsgroups_test.data)\n",
        "accuracy = metrics.accuracy_score(newsgroups_test.target, predicted)\n",
        "print(f\"정확도: {accuracy:.4f}\")\n",
        "\n",
        "# 5. 분류 보고서 출력\n",
        "print(metrics.classification_report(newsgroups_test.target, predicted, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# 6. 혼동 행렬 시각화\n",
        "conf_matrix = metrics.confusion_matrix(newsgroups_test.target, predicted)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(conf_matrix, annot=True, xticklabels=newsgroups_test.target_names, yticklabels=newsgroups_test.target_names, fmt='d', cmap='Blues')\n",
        "plt.xlabel('예측 레이블')\n",
        "plt.ylabel('실제 레이블')\n",
        "plt.title('혼동 행렬')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SD2RugX0Vff2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP에서 자주 활용되는 다음과 같은 기술을 실습할 수 있습니다:\n",
        "\n",
        "단어 벡터화 (TF-IDF, CountVectorizer 등)\n",
        "\n",
        "차원 축소 (PCA, TruncatedSVD)\n",
        "\n",
        "클러스터링 (KMeans 등)\n",
        "\n",
        "분류기 (Naive Bayes, SVM, Logistic Regression 등)"
      ],
      "metadata": {
        "id": "-4HzXSrTX5cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 주요 설명:\n",
        "CountVectorizer: 단어의 출현 빈도를 기반으로 텍스트를 수치 벡터로 변환\n",
        "\n",
        "TfidfTransformer: 단순한 카운트 대신 TF-IDF로 가중치 부여\n",
        "\n",
        "MultinomialNB: 다항 분포 기반의 나이브 베이즈 모델 (텍스트 분류에 적합)\n",
        "\n",
        "Pipeline: 전처리 + 모델을 하나의 흐름으로 묶어서 사용\n",
        "\n",
        "혼동 행렬: 분류 성능을 시각적으로 확인 가능\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "\n",
        "TF-IDF의 full name은:\n",
        "\n",
        "Term Frequency – Inverse Document Frequency\n",
        "\n",
        "각 구성 요소 설명:\n",
        "Term Frequency (TF)\n",
        "\n",
        "특정 문서에서 특정 단어가 얼마나 자주 등장하는지 나타냅니다.\n",
        "\n",
        "예시: 문서 A에서 \"space\"가 3번 나왔다면 → TF = 3\n",
        "\n",
        "Inverse Document Frequency (IDF)\n",
        "\n",
        "전체 문서 집합에서 특정 단어가 얼마나 희귀한지를 측정합니다.\n",
        "\n",
        "자주 등장하는 단어(예: \"the\", \"is\")에는 낮은 가중치를,\n",
        "드물게 등장하는 단어(예: \"galaxy\")에는 높은 가중치를 부여합니다.\n",
        "\n",
        "IDF\n",
        "(\n",
        "𝑡\n",
        ")\n",
        "=\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑁\n",
        "1\n",
        "+\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "IDF(t)=log(\n",
        "1+n\n",
        "t\n",
        "​\n",
        "\n",
        "N\n",
        "​\n",
        " )\n",
        "𝑁\n",
        "N: 전체 문서 수\n",
        "\n",
        "𝑛\n",
        "𝑡\n",
        "n\n",
        "t\n",
        "​\n",
        " : 단어\n",
        "𝑡\n",
        "t가 등장한 문서 수\n",
        "\n",
        "TF-IDF\n",
        "\n",
        "TF-IDF\n",
        "(\n",
        "𝑡\n",
        ",\n",
        "𝑑\n",
        ")\n",
        "=\n",
        "TF\n",
        "(\n",
        "𝑡\n",
        ",\n",
        "𝑑\n",
        ")\n",
        "×\n",
        "IDF\n",
        "(\n",
        "𝑡\n",
        ")\n",
        "TF-IDF(t,d)=TF(t,d)×IDF(t)\n",
        "단어가 특정 문서에 많이 나오고 전체적으로는 드물수록 큰 값을 가집니다.\n",
        "\n",
        "이 값은 문서 내 단어의 중요도를 수치화할 수 있어, 텍스트 분류, 검색엔진, 문서 요약 등에 널리 사용됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "kGw_MEbxWHg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 축 확인:\n",
        "\n",
        "가로축 (예측 레이블): 모델이 예측한 클래스를 나타냅니다. 여기서는 'alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc'의 네 가지 클래스로 예측했습니다.\n",
        "세로축 (실제 레이블): 데이터의 실제 클래스를 나타냅니다. 세로축의 순서는 가로축과 동일하게 배열됩니다.\n",
        "2. 각 셀의 의미:\n",
        "\n",
        "각 셀의 숫자는 해당되는 경우의 데이터 개수를 나타냅니다.\n",
        "\n",
        "대각선 셀 (True Positives & True Negatives):\n",
        "\n",
        "(1행 1열, 238): 실제 클래스가 'alt.atheism'이고 모델도 'alt.atheism'으로 정확하게 예측한 데이터의 개수입니다. (True Positive for 'alt.atheism')\n",
        "(2행 2열, 343): 실제 클래스가 'comp.graphics'이고 모델도 'comp.graphics'으로 정확하게 예측한 데이터의 개수입니다. (True Positive for 'comp.graphics')\n",
        "(3행 3열, 364): 실제 클래스가 'sci.space'이고 모델도 'sci.space'으로 정확하게 예측한 데이터의 개수입니다. (True Positive for 'sci.space')\n",
        "(4행 4열, 18): 실제 클래스가 'talk.religion.misc'이고 모델도 'talk.religion.misc'으로 정확하게 예측한 데이터의 개수입니다. (True Positive for 'talk.religion.misc')\n",
        "일반적으로 대각선 셀의 값이 클수록 모델의 성능이 좋습니다.\n",
        "비대각선 셀 (False Positives & False Negatives):\n",
        "\n",
        "(1행 2열, 8): 실제 클래스는 'alt.atheism'이지만 모델이 'comp.graphics'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'alt.atheism', False Positive for 'comp.graphics')\n",
        "(1행 3열, 68): 실제 클래스는 'alt.atheism'이지만 모델이 'sci.space'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'alt.atheism', False Positive for 'sci.space')\n",
        "(1행 4열, 5): 실제 클래스는 'alt.atheism'이지만 모델이 'talk.religion.misc'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'alt.atheism', False Positive for 'talk.religion.misc')\n",
        "(2행 1열, 8): 실제 클래스는 'comp.graphics'이지만 모델이 'alt.atheism'으로 잘못 예측한 데이터의 개수입니다. (False Negative for 'comp.graphics', False Positive for 'alt.atheism')\n",
        "(2행 3열, 38): 실제 클래스는 'comp.graphics'이지만 모델이 'sci.space'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'comp.graphics', False Positive for 'sci.space')\n",
        "(2행 4열, 0): 실제 클래스는 'comp.graphics'이지만 모델이 'talk.religion.misc'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'comp.graphics', False Positive for 'talk.religion.misc')\n",
        "(3행 1열, 12): 실제 클래스는 'sci.space'이지만 모델이 'alt.atheism'으로 잘못 예측한 데이터의 개수입니다. (False Negative for 'sci.space', False Positive for 'alt.atheism')\n",
        "(3행 2열, 18): 실제 클래스는 'sci.space'이지만 모델이 'comp.graphics'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'sci.space', False Positive for 'comp.graphics')\n",
        "(3행 4열, 0): 실제 클래스는 'sci.space'이지만 모델이 'talk.religion.misc'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'sci.space', False Positive for 'talk.religion.misc')\n",
        "(4행 1열, 174): 실제 클래스는 'talk.religion.misc'이지만 모델이 'alt.atheism'으로 잘못 예측한 데이터의 개수입니다. (False Negative for 'talk.religion.misc', False Positive for 'alt.atheism')\n",
        "(4행 2열, 11): 실제 클래스는 'talk.religion.misc'이지만 모델이 'comp.graphics'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'talk.religion.misc', False Positive for 'comp.graphics')\n",
        "(4행 3열, 48): 실제 클래스는 'talk.religion.misc'이지만 모델이 'sci.space'로 잘못 예측한 데이터의 개수입니다. (False Negative for 'talk.religion.misc', False Positive for 'sci.space')\n",
        "비대각선 셀의 값이 작을수록 모델의 성능이 좋습니다.\n",
        "3. 모델 성능 평가 지표 계산 (혼동 행렬 기반):"
      ],
      "metadata": {
        "id": "IJ_1bau_W0YK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-8kc2PhWIKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "분석 결과 해석:\n",
        "\n",
        "모델은 'comp.graphics'와 'sci.space' 클래스를 비교적 잘 예측하는 것으로 보입니다 (대각선 값이 크고, 해당 행/열의 오분류 값이 작음).\n",
        "'alt.atheism' 클래스는 'sci.space'로 잘못 분류되는 경우가 많습니다 (68개).\n",
        "'talk.religion.misc' 클래스의 재현율이 매우 낮습니다 (0.082). 이는 실제 'talk.religion.misc' 클래스에 속하는 데이터를 모델이 제대로 예측하지 못하는 경우가 많다는 의미입니다. 특히 'alt.atheism'으로 잘못 분류하는 경우가 174개로 매우 많습니다.\n",
        "5. 추가 분석 방향:\n",
        "\n",
        "클래스 불균형 확인: 각 클래스의 실제 데이터 개수를 확인하여 특정 클래스의 데이터가 부족하여 모델 학습이 제대로 이루어지지 않았는지 확인할 수 있습니다.\n",
        "오분류 원인 분석: 특정 클래스 간의 오분류가 많이 발생하는 이유를 데이터의 특성이나 모델의 한계점에서 찾아볼 수 있습니다. 예를 들어, 'talk.religion.misc'와 'alt.atheism'의 텍스트 내용이 유사하여 모델이 혼동할 수 있습니다.\n",
        "모델 개선: 혼동 행렬 분석 결과를 바탕으로 데이터 전처리, 특성 선택, 모델 튜닝 등을 통해 모델 성능을 개선할 수 있습니다."
      ],
      "metadata": {
        "id": "AGkqIMScWvOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# 샘플 텍스트 데이터\n",
        "texts = [\n",
        "    \"spam spam spam spam love win winner prize\",\n",
        "    \"spam spam money offer limited deal\",\n",
        "    \"free money prize claim winner now\",\n",
        "    \"limited time offer win cash prize\",\n",
        "    \"you won a prize claim now\",  # spam\n",
        "\n",
        "    \"hello how are you doing today\",\n",
        "    \"let's meet for lunch and coffee\",\n",
        "    \"happy birthday have a great day\",\n",
        "    \"see you at the meeting tomorrow\",\n",
        "    \"thank you for your help\"  # ham\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1: spam, 0: ham\n",
        "\n",
        "# 벡터화\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = np.array(labels)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Multinomial Naive Bayes 훈련\n",
        "clf = MultinomialNB()\n",
        "clf.partial_fit(X[:1], y[:1], classes=np.array([0, 1]))  # 초기화\n",
        "\n",
        "# 애니메이션용 데이터 준비\n",
        "steps = []\n",
        "for i in range(2, len(X.toarray()) + 1):\n",
        "    clf.partial_fit(X[:i], y[:i])\n",
        "    steps.append(clf.feature_log_prob_.copy())\n",
        "\n",
        "# 애니메이션 생성\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bar_container = ax.bar(feature_names, np.exp(steps[0][1]), color='tomato')\n",
        "ax.set_ylim(0, 0.4)\n",
        "ax.set_title(\"Word Probabilities for Class 'Spam'\")\n",
        "ax.set_ylabel(\"Probability\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "def update(frame):\n",
        "    probs = np.exp(steps[frame][1])  # spam class\n",
        "    for bar, new_height in zip(bar_container, probs):\n",
        "        bar.set_height(new_height)\n",
        "    ax.set_title(f\"Word Probabilities for Class 'Spam' (Step {frame+2})\")\n",
        "    return bar_container\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(steps), interval=800, blit=False)\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "hTreNcHaWv-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}