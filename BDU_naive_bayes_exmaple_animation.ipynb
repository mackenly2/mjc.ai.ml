{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNjP4fmpgjb2Rc7e1aNeoUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjungweapon/mjc.ai.ml/blob/BDU/BDU_naive_bayes_exmaple_animation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì—¬ê¸°ì„œëŠ” **Scikit-learnì˜ MultinomialNB (Multinomial Naive Bayes)ë¥¼** ì‚¬ìš©í•´, ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ì™€ ìœ ì‚¬í•œ ì•„ì£¼ ì‰¬ìš´ ì˜ˆì œë¥¼ ë³´ì—¬ë“œë¦´ê²Œìš”. í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë©°, íŒŒì´í”„ë¼ì¸ê¹Œì§€ í¬í•¨ë˜ì–´ ìˆì–´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "kQaw9nlFBq51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "< ì ‘ê·¼ ë°©ë²• >\n",
        "\n",
        "ì£¼ìš” í¬ì¸íŠ¸\n",
        "CountVectorizer()ê°€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë¹ˆë„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "MultinomialNB()ëŠ” ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "make_pipelineì€ ë²¡í„°í™” â†’ ë¶„ë¥˜ê¸°ë¥¼ í•œ ë²ˆì— ë¬¶ì–´ì¤ë‹ˆë‹¤.\n",
        "\n",
        "ê²°ê³¼ëŠ” confusion matrixì™€ precision, recall, f1-scoreë¡œ í‰ê°€ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "g9EbwIuuG6U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. import í•˜ê¸°"
      ],
      "metadata": {
        "id": "PshG9nuKCq4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "Az-fs-7wCuPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. data ëª¨ìœ¼ê¸°"
      ],
      "metadata": {
        "id": "7o37opk1CyaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ìƒ˜í”Œ ë°ì´í„°\n",
        "texts = [\n",
        "    \"Win money now!\", \"Claim your prize\", \"Hello friend\", \"Let's have lunch\",\n",
        "    \"Congratulations, you have won!\", \"Are we still meeting tomorrow?\",\n",
        "    \"This is not spam\", \"Free entry in 2 a weekly competition to win!\",\n",
        "    \"Call me later\", \"Urgent! You have won a 1 week free membership\"\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1, 0, 0, 1, 0, 1]  # 1: spam, 0: not spam"
      ],
      "metadata": {
        "id": "_BgZxLxyC0dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. data ë¶„ë¦¬"
      ],
      "metadata": {
        "id": "YDBWlMV2C6oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. ë°ì´í„° ë¶„ë¦¬\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "VeW5Ev8qC8o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. pipe line  ë§Œë“¤ê¸°"
      ],
      "metadata": {
        "id": "6fg1FGLJDBCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. íŒŒì´í”„ë¼ì¸: í…ìŠ¤íŠ¸ ë²¡í„°í™” + Naive Bayes ë¶„ë¥˜\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())"
      ],
      "metadata": {
        "id": "pln4dPFUDEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make_pipeline í•¨ìˆ˜ë¥¼ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ ì ìš© ì‹œ ì‚¬ìš©í•˜ì…¨êµ°ìš”! ì—¬ê¸°ì„œ pipelineì´ë¼ëŠ” ê²ƒì€ ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„ì™€ ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë„êµ¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§ˆì¹˜ ê³µì¥ì˜ ì¡°ë¦½ ë¼ì¸ì²˜ëŸ¼, ë°ì´í„°ê°€ íŒŒì´í”„ë¼ì¸ì„ ë”°ë¼ íë¥´ë©´ì„œ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ìµœì¢… ê²°ê³¼(ì˜ˆì¸¡)ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê³¼ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "sklearn.pipeline.Pipelineì˜ ì—­í• :\n",
        "\n",
        "make_pipeline í•¨ìˆ˜ë‚˜ Pipeline í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "í¸ë¦¬í•œ ëª¨ë¸ êµ¬ì¶•: ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì‘ì—…ì„ í•˜ë‚˜ì˜ ê°ì²´ë¡œ ë¬¶ì–´ ì½”ë“œì˜ ê°€ë…ì„±ì„ ë†’ì´ê³  ëª¨ë¸ êµ¬ì¶• ê³¼ì •ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì„± ìŠ¤ì¼€ì¼ë§, ì°¨ì› ì¶•ì†Œ, ëª¨ë¸ í•™ìŠµ ë“±ì˜ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì •ì˜í•˜ê³  í•œ ë²ˆì˜ fit, predict í˜¸ì¶œë¡œ ì „ì²´ ê³¼ì •ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "êµì°¨ ê²€ì¦ì˜ ìš©ì´ì„±: íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë©´ êµì°¨ ê²€ì¦ ì‹œ ê° í´ë“œë§ˆë‹¤ ë™ì¼í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ëˆ„ì¶œ(data leakage)ì„ ë°©ì§€í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë” ì •í™•í•˜ê²Œ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ë§Œì•½ íŒŒì´í”„ë¼ì¸ ì—†ì´ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•œë‹¤ë©´, ê° í´ë“œë§ˆë‹¤ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë°˜ë³µí•´ì•¼ í•˜ê³  ì‹¤ìˆ˜í•  ê°€ëŠ¥ì„±ë„ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "ëª¨ë¸ ë°°í¬ ë° ê´€ë¦¬ì˜ ìš©ì´ì„±: í•™ìŠµëœ íŒŒì´í”„ë¼ì¸ ê°ì²´ë¥¼ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì™€ì„œ ìƒˆë¡œìš´ ë°ì´í„°ì— ë™ì¼í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹œ í›„ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ ë°°í¬ ë° ê´€ë¦¬ ì¸¡ë©´ì—ì„œ ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
        "\n",
        "ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ ì ìš© ì‹œ íŒŒì´í”„ë¼ì¸ì˜ ì˜ˆì‹œ:\n",
        "\n",
        "ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì„ ì ìš©í•  ë•Œ, ì¢…ì¢… ë‹¤ìŒê³¼ ê°™ì€ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš°:\n",
        "CountVectorizer ë˜ëŠ” TfidfVectorizer: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¨ì–´ ë¹ˆë„ ë˜ëŠ” TF-IDF ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "Feature Scaling (ë“œë¬¼ì§€ë§Œ): ê²½ìš°ì— ë”°ë¼ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ìˆ˜ì¹˜í˜• ë°ì´í„°ì˜ ê²½ìš°:\n",
        "StandardScaler ë˜ëŠ” MinMaxScaler: íŠ¹ì„±ë“¤ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•˜ì—¬ ëª¨ë¸ í•™ìŠµì˜ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n",
        "PCA (Principal Component Analysis): ì°¨ì›ì„ ì¶•ì†Œí•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ì´ëŸ¬í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ì™€ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¶ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Python\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë°ì´í„°ì— Multinomial Naive Bayes ëª¨ë¸ ì ìš©\n",
        "text_pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# ìˆ˜ì¹˜í˜• ë°ì´í„°ì— Gaussian Naive Bayes ëª¨ë¸ ì ìš© (ìŠ¤ì¼€ì¼ë§ í¬í•¨)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "numeric_pipeline = make_pipeline(StandardScaler(), GaussianNB())\n",
        "ìœ„ì˜ ì˜ˆì‹œì—ì„œ text_pipelineì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ TF-IDFë¡œ ë³€í™˜í•œ í›„ Multinomial Naive Bayes ëª¨ë¸ë¡œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ í•˜ë‚˜ì˜ ê°ì²´ë¡œ ë¬¶ì—ˆìŠµë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ numeric_pipelineì€ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ í‘œì¤€ ìŠ¤ì¼€ì¼ë§í•œ í›„ Gaussian Naive Bayes ëª¨ë¸ë¡œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë¬¶ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ì œ text_pipelineì´ë‚˜ numeric_pipeline ê°ì²´ì— ëŒ€í•´ fit ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´, íŒŒì´í”„ë¼ì¸ ë‚´ì˜ ëª¨ë“  ë‹¨ê³„ê°€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. predict ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë™ì¼í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹œ í›„ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "ê²°ë¡ ì ìœ¼ë¡œ, make_pipelineì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì„ ì ìš©í•  ë•Œ, íŒŒì´í”„ë¼ì¸ì€ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í•™ìŠµê¹Œì§€ì˜ ì¼ë ¨ì˜ ê³¼ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë§¤ìš° ìœ ìš©í•œ ë„êµ¬ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "d05Ke38DE8s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. í•™ìŠµ í•˜ê¸°"
      ],
      "metadata": {
        "id": "iAr3Xr8GDutL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. í•™ìŠµ\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "lUMWspJ6DxgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. ì˜ˆì¸¡ í•˜ê¸°"
      ],
      "metadata": {
        "id": "g-gZficeD04O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. ì˜ˆì¸¡\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "zOkyHsmVD2S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. í‰ê°€ í•˜ê¸°"
      ],
      "metadata": {
        "id": "FjZtjNCND5J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. í‰ê°€\n",
        "print(\"=== ì˜ˆì¸¡ ê²°ê³¼ ===\")\n",
        "print(y_pred)\n",
        "print(\"\\n=== ì •í™•ë„ í‰ê°€ ===\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n=== ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Not Spam\", \"Spam\"]))\n"
      ],
      "metadata": {
        "id": "1_kQBjWVD7Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "animation\n"
      ],
      "metadata": {
        "id": "2Bw8yPbiCV13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 1. ìƒ˜í”Œ í…ìŠ¤íŠ¸ & ë ˆì´ë¸”\n",
        "texts = [\n",
        "    \"Win money now!\", \"Claim your prize\", \"Hello friend\", \"Let's have lunch\",\n",
        "    \"Congratulations, you have won!\", \"Are we still meeting tomorrow?\",\n",
        "    \"This is not spam\", \"Free entry in 2 a weekly competition to win!\",\n",
        "    \"Call me later\", \"Urgent! You have won a 1 week free membership\"\n",
        "]\n",
        "labels = [1,1,0,0,1,0,0,1,0,1]  # 1: spam, 0: not spam\n",
        "\n",
        "# 2. train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# 3. ë²¡í„°í™” & PCA (ì‹œê°í™”ìš©)\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train).toarray()   # <-- toarray() ì¶”ê°€\n",
        "texts_vec    = vectorizer.transform(texts).toarray()\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "texts_pca = pca.fit_transform(texts_vec)\n",
        "\n",
        "# 4. partial-fit ê°€ëŠ¥í•œ NB ëª¨ë¸ ì¤€ë¹„\n",
        "partial_model = MultinomialNB()\n",
        "n_train = len(X_train_vec)\n",
        "\n",
        "# 5. ì• ë‹ˆë©”ì´ì…˜ ì¤€ë¹„\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "colors = ['blue','red']  # 0:not spam, 1:spam\n",
        "\n",
        "# ëª¨ë¸ì„ fitìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "partial_model.fit(X_train_vec, y_train)\n",
        "\n",
        "def init():\n",
        "    ax.clear()\n",
        "    ax.set_xlim(texts_pca[:,0].min()-1, texts_pca[:,0].max()+1)\n",
        "    ax.set_ylim(texts_pca[:,1].min()-1, texts_pca[:,1].max()+1)\n",
        "    ax.set_title(\"Naive Bayes Learning Progress\")\n",
        "    ax.grid(True)\n",
        "    return []\n",
        "\n",
        "def update(frame):\n",
        "    ax.clear()\n",
        "    # 0í”„ë ˆì„: ë¯¸í•™ìŠµ ìƒíƒœ\n",
        "    if frame > 0:\n",
        "        Xi = X_train_vec[frame-1:frame]\n",
        "        yi = [y_train[frame-1]]\n",
        "        # ì²« í˜¸ì¶œì—ë§Œ classes ì¸ì ì „ë‹¬\n",
        "        partial_model.partial_fit(Xi, yi, classes=np.array([0,1]))\n",
        "    # í˜„ì¬ ëª¨ë¸ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡\n",
        "    preds = partial_model.predict(texts_vec)\n",
        "    # ì‹œê°í™”\n",
        "    for i, (x,y) in enumerate(texts_pca):\n",
        "        ax.scatter(x, y,\n",
        "                   c=colors[preds[i]],\n",
        "                   edgecolor='k',\n",
        "                   s=100,\n",
        "                   alpha=0.6)\n",
        "        ax.text(x+0.02, y+0.02, str(i), fontsize=8)\n",
        "    ax.set_title(f\"Step {frame}/{n_train}\")\n",
        "    ax.set_xlim(texts_pca[:,0].min()-1, texts_pca[:,0].max()+1)\n",
        "    ax.set_ylim(texts_pca[:,1].min()-1, texts_pca[:,1].max()+1)\n",
        "    ax.grid(True)\n",
        "    return []\n",
        "\n",
        "# 6. ì• ë‹ˆë©”ì´ì…˜ ì‹¤í–‰\n",
        "ani = FuncAnimation(\n",
        "    fig, update, frames=n_train+1,\n",
        "    init_func=init, interval=800, repeat=False\n",
        ")\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "btmL06NwCXSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WK0dxsFxEW1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colabì—ì„œ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì„¸ìš”.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 1) 2D í•©ì„± ë°ì´í„° ìƒì„±\n",
        "np.random.seed(0)\n",
        "X, y = make_classification(\n",
        "    n_samples=200, n_features=2,\n",
        "    n_redundant=0, n_informative=2,\n",
        "    n_clusters_per_class=1, random_state=42\n",
        ")\n",
        "\n",
        "# 2) í•™ìŠµ ìˆœì„œ ì •í•˜ê¸° (ì—¬ê¸°ì„  ì• 50ê°œë§Œ ì‚¬ìš©)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "indices = indices[:50]\n",
        "n_steps = len(indices)\n",
        "\n",
        "# 3) Gaussian Naive Bayes ëª¨ë¸ ì¤€ë¹„\n",
        "model = GaussianNB()\n",
        "\n",
        "# 4) ê²°ì • ê²½ê³„ ê·¸ë¦´ ë©”ì‰¬ ê·¸ë¦¬ë“œ ì¤€ë¹„\n",
        "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 100),\n",
        "    np.linspace(y_min, y_max, 100)\n",
        ")\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# 5) í”Œë¡¯ ì„¸íŒ…\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "\n",
        "def update(step):\n",
        "    ax.clear()\n",
        "    idx = indices[step]\n",
        "    xi, yi = X[idx:idx+1], y[idx:idx+1]\n",
        "\n",
        "    # ì²« step ì—ë§Œ classes ì§€ì •\n",
        "    if step == 0:\n",
        "        model.partial_fit(xi, yi, classes=np.unique(y))\n",
        "    else:\n",
        "        model.partial_fit(xi, yi)\n",
        "\n",
        "    # ê²½ê³„ ê·¸ë¦¬ê¸°\n",
        "    Z = model.predict(grid).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "\n",
        "    # ì›ë³¸ ë°ì´í„° ì „ì²´ë¥¼ í˜„ì¬ ì˜ˆì¸¡ ìƒ‰ìœ¼ë¡œ í‘œì‹œ\n",
        "    preds = model.predict(X)\n",
        "    ax.scatter(X[:,0], X[:,1], c=preds, cmap='RdBu', edgecolor='k')\n",
        "\n",
        "    ax.set_title(f'Gaussian NB Learning â€” Step {step+1}/{n_steps}')\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.grid(True)\n",
        "\n",
        "# 6) ì• ë‹ˆë©”ì´ì…˜ ìƒì„± & Colabì—ì„œ HTML ë¡œ í‘œì‹œ\n",
        "ani = FuncAnimation(fig, update, frames=n_steps, interval=500, repeat=False)\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "8XGiSWB_FoRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive baysian ì— ëŒ€í•´ì„œ  êµ¬ì²´ì ì¸ ì˜ˆë¡œ ë‹¤ì‹œ ì„¤ëª…í•´ ë³´ì\n",
        "\n",
        "ê°€ìƒì˜ ì˜ˆì‹œ: ë‚ ì”¨ì— ë”°ë¥¸ í™œë™ ë¶„ë¥˜\n",
        "\n",
        "ìš°ë¦¬ê°€ \"ë§‘ìŒ\", \"íë¦¼\", \"ë¹„\" ì„¸ ê°€ì§€ ë‚ ì”¨ ì¡°ê±´ì— ë”°ë¼ ì‚¬ëŒë“¤ì´ \"ê³µì›\", \"ì‹¤ë‚´\", \"ì§‘\" ì¤‘ ì–´ë–¤ í™œë™ì„ í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ìƒê°í•´ ë³´ì.\n",
        "\n",
        "ìš°ë¦¬ì˜ ëª©ì  ??  **ë‚ ì”¨ ì¡°ê±´ì— ë”°ë¼ ì–´ë–¤ í™œë™ì„ í•  ê²ƒì¸ì§€ë¥¼ ì˜ˆì¸¡**"
      ],
      "metadata": {
        "id": "f7XulpFbI_la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "ë‚ ì”¨,í™œë™\n",
        "ë§‘ìŒ,ê³µì›\n",
        "ë§‘ìŒ,ê³µì›\n",
        "íë¦¼,ì‹¤ë‚´\n",
        "ë¹„,ì§‘\n",
        "ë¹„,ì§‘\n",
        "ë§‘ìŒ,ê³µì›\n",
        "íë¦¼,ì‹¤ë‚´\n",
        "íë¦¼,ì§‘\n",
        "ë¹„,ì§‘\n",
        "ë§‘ìŒ,ê³µì›\n",
        "```\n"
      ],
      "metadata": {
        "id": "Rt6OJUIHJZy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ„ ì •ë³´ë¡œ ë¶€í„°,  í•„ìš”í•œ í™•ë¥ ì„ ê³„ì‚°í•´ ë³´ì (ì†ìœ¼ë¡œ)\n",
        "\n",
        "1. ì‚¬ì „ í™•ë¥  (prior Probability )\n",
        "  \n",
        "     P(ê³µì›) = 4 / 10 = 0.4\n",
        "     P(ì§‘)   = 4 / 10 = 0.4\n",
        "     P(ì‹¤ë‚´) = 2/ 10 = 0.2\n",
        "\n",
        "2. ê° ë‚ ì”¨ ì¡°ê±´ í•˜ì—ì„œì˜ í™œë™ ( Likelihood ,ìš°ë„) í™•ë£”\n",
        "   \n",
        "     P(ë§‘ìŒ|ê³µì›) = 4/4 = 1.0  ( ê³µì› í™œë™ ì¤‘, ë‚ ì”¨ ë§‘ì€ ì¡°ê±´ì—ì„œ )\n",
        "     P(íë¦¼|ê³µì›) = 0 / 4 = 0.0\n",
        "     P(ë¹„|ê³µì›) =  0 / 4 = 0.0\n",
        "\n",
        "     P(ë§‘ìŒ|ì‹¤ë‚´)   0 / 2 = 0.0\n",
        "     P(íë¦¼|ì‹¤ë‚´)   2 / 2 = 1\n",
        "     P(ë¹„|ì‹¤ë‚´)     0 / 2 = 0.0\n",
        "\n",
        "     P(ë§‘ìŒ|ì§‘)     0 / 4\n",
        "     P(íë¦¼|ì§‘)     1 / 4   = 0.25\n",
        "     P(ë¹„|ì§‘)       3/ 4    = 0.75\n",
        "\n",
        "3. ì´ì œ ì‚¬í›„ í™•ë¥  ê³„ì‚°í•˜ì. P(H|D) = P(H) x P(D|H)  / P(D)  // HëŠ” ê°€ì„¤, DëŠ” data\n",
        "\n",
        "   ìƒˆë¡œìš´ ë‚ ì”¨ ì¡°ê±´ì´ 'íë¦¼' ìœ¼ë¡œ ì£¼ì–´ ì¡Œì„ ë•Œì˜ ì‚¬í›„  í™•ë¥  ì€ ( P(D)ëŠ” ë™ì¼ ì¡°ê±´ì´ë¼ ìƒëµ, ì–´ì°¨í”¼ í¬ê¸°(í™•ë¥ ) ë¹„êµ ì´ë¯€ë¡œ )\n",
        "\n",
        "   P(ê³µì›) x P(íë¦¼|ê³µì›)   = 0.4 x 0.0 = 0.0\n",
        "   P(ì§‘) X P(íë¦¼|ì§‘)     =  0.4 x 0.25 = 0.1\n",
        "   P(ì‹¤ë‚´) x P(íë¦¼|ì‹¤ë‚´) =   0.2 x 1.0 = 0.2\n",
        "\n",
        "\n",
        "#   ==> ê²°ê³¼ ,  ìœ„ ë‚´ìš©ì„ ë³´ë©´  ì‚¬í›„ í™•ë¥ ì„ ë¹„êµí•´ ë³´ë©´, \"íë¦¼\" ë‚ ì”¨ì—ëŠ” \"ì‹¤ë‚´\" í™œë™ì„ í•  í™•ë¥ ì´ ê°€ì¥ ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì€ \"íë¦¼\"ì´ë¼ëŠ” ì…ë ¥ì— ëŒ€í•´ \"ì‹¤ë‚´\"ë¼ê³  ì˜ˆì¸¡í•  ê²ƒ\n"
      ],
      "metadata": {
        "id": "kK2pfP-7JkNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1) ë°ì´í„° ì¤€ë¹„\n",
        "data = {\n",
        "    'ë‚ ì”¨': ['ë§‘ìŒ','ë§‘ìŒ','íë¦¼','ë¹„','ë¹„','ë§‘ìŒ','íë¦¼','íë¦¼','ë¹„','ë§‘ìŒ'],\n",
        "    'í™œë™': ['ê³µì›','ê³µì›','ì‹¤ë‚´','ì§‘','ì§‘','ê³µì›','ì‹¤ë‚´','ì§‘','ì§‘','ê³µì›']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2) ì¸ì½”ë”©\n",
        "le_weather  = LabelEncoder().fit(df['ë‚ ì”¨'])\n",
        "le_activity = LabelEncoder().fit(df['í™œë™'])\n",
        "\n",
        "X = le_weather.transform(df['ë‚ ì”¨']).reshape(-1,1)\n",
        "y = le_activity.transform(df['í™œë™'])\n",
        "\n",
        "# 3) ëª¨ë¸ í•™ìŠµ\n",
        "model = CategoricalNB()   ### ì¹´í…Œê³ ë¦¬ ë¶„í¬(categorical distribution) ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •\n",
        "model.fit(X, y)\n",
        "\n",
        "# 4) ì‚¬ì „í™•ë¥  ì¶œë ¥ (ë¡œê·¸â†’ì§€ìˆ˜í™”)\n",
        "log_priors = model.class_log_prior_\n",
        "priors     = np.exp(log_priors)\n",
        "print(\"í´ë˜ìŠ¤ë³„ ì‚¬ì „í™•ë¥  (prior):\")\n",
        "for cls_idx, prop in enumerate(priors):\n",
        "    print(f\"  {le_activity.inverse_transform([cls_idx])[0]}: {prop:.2f}\")\n",
        "\n",
        "# ë˜ëŠ” class_count_ ì‚¬ìš©\n",
        "counts = model.class_count_\n",
        "print(\"\\nclass_count_ ê¸°ë°˜ ì‚¬ì „í™•ë¥ :\")\n",
        "for cls_idx, cnt in enumerate(counts):\n",
        "    print(f\"  {le_activity.inverse_transform([cls_idx])[0]}: {cnt/counts.sum():.2f}\")\n",
        "\n",
        "# 5) ì¡°ê±´ë¶€ í™•ë¥  ì¶œë ¥\n",
        "print(\"\\në‚ ì”¨ë³„ í™œë™ ì¡°ê±´ë¶€ í™•ë¥  P(í™œë™|ë‚ ì”¨):\")\n",
        "# feature_log_prob_[0]ì€ ì²« ë²ˆì§¸(ê·¸ë¦¬ê³  ìœ ì¼í•œ) íŠ¹ì„±ì— ëŒ€í•œ ë¡œê·¸ìš°ë„ ë°°ì—´:\n",
        "# shape = (n_classes, n_categories_of_ë‚ ì”¨)\n",
        "log_likelihood = model.feature_log_prob_[0]\n",
        "for cat_idx, weather in enumerate(le_weather.classes_):\n",
        "    print(f\"\\në‚ ì”¨ = {weather}\")\n",
        "    for cls_idx, activity in enumerate(le_activity.classes_):\n",
        "        prob = np.exp(log_likelihood[cls_idx, cat_idx])\n",
        "        print(f\"  P(í™œë™={le_activity.inverse_transform([cls_idx])[0]} | ë‚ ì”¨) = {prob:.2f}\")\n"
      ],
      "metadata": {
        "id": "rPPH4oNXJcAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì½”ë“œ ì„¤ëª…:\n",
        "\n",
        "ë°ì´í„° ì¤€ë¹„: íŒë‹¤ìŠ¤ DataFrameì„ ì´ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬: 'ë‚ ì”¨'ë¥¼ íŠ¹ì„±ìœ¼ë¡œ, 'í™œë™'ì„ íƒ€ê²Ÿìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Label Encoding: ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ìˆ«ìí˜• ì…ë ¥ì„ ë°›ê¸° ë•Œë¬¸ì—, LabelEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤. \"ê³µì›\" -> 0, \"ì‹¤ë‚´\" -> 1, \"ì§‘\" -> 2 ì™€ ê°™ì´ ì¸ì½”ë”©ë©ë‹ˆë‹¤. \"ë§‘ìŒ\" -> 0, \"ë¹„\" -> 1, \"íë¦¼\" -> 2 ì™€ ê°™ì´ ë‚ ì”¨ë„ ì¸ì½”ë”©ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ: CategoricalNB ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³ \n",
        "í•™ìŠµ ë°ì´í„°ë¡œ fit ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
        "\n",
        "(*) CategoricalNBëŠ” ë²”ì£¼í˜• íŠ¹ì„±ì„ ë‹¤ë£¨ëŠ” ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡: ìƒˆë¡œìš´ ë‚ ì”¨ \"íë¦¼\"ì„ ì¸ì½”ë”©í•˜ì—¬ ëª¨ë¸ì˜ predict ë©”ì„œë“œì— ì…ë ¥í•˜ê³ , ì˜ˆì¸¡ëœ í™œë™ì„ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "í™•ë¥  í™•ì¸:\n",
        "model.class_prior_: í•™ìŠµëœ ê° í´ë˜ìŠ¤ì˜ ì‚¬ì „ í™•ë¥ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (ì˜ˆ: [0.4, 0.2, 0.4] -> ê³µì›, ì‹¤ë‚´, ì§‘ ìˆœì„œ)\n",
        "model.feature_log_prob_: ê° í´ë˜ìŠ¤ ë‚´ì—ì„œ ê° íŠ¹ì„±(ë‚ ì”¨)ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ë¡œê·¸ ê°’ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë¡œê·¸ ê°’ì„ ì‚¬ìš©í•˜ë©´ ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê³„ì‚°ì˜ ì•ˆì •ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "model.category_count_[i] / model.class_count_[i]: ê° í´ë˜ìŠ¤ ë‚´ì—ì„œ íŠ¹ì • ë‚ ì”¨ê°€ ë‚˜íƒ€ë‚œ íšŸìˆ˜ë¥¼ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì´ ë°ì´í„° ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì‹¤ì œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "ê²°ê³¼ ë¶„ì„:\n",
        "\n",
        "ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ì—ì„œ \"íë¦¼\" ë‚ ì”¨ì— ëŒ€í•´ \"ì‹¤ë‚´\" í™œë™ì´ ì˜ˆì¸¡ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. model.class_prior_ë¥¼ í†µí•´ ê° í™œë™ì˜ ì‚¬ì „ í™•ë¥ ì„, model.feature_log_prob_ì™€ ì‹¤ì œ ë¹„ìœ¨ ê³„ì‚°ì„ í†µí•´ ê° ë‚ ì”¨ ì¡°ê±´ í•˜ì—ì„œ íŠ¹ì • í™œë™ì´ ë‚˜íƒ€ë‚  ì¡°ê±´ë¶€ í™•ë¥ (ìš°ë„)ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ ì½”ë“œë¥¼ í†µí•´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì•Œê³ ë¦¬ì¦˜ì´ í•™ìŠµ ë°ì´í„°ì—ì„œ í™•ë¥  ì •ë³´ë¥¼ í•™ìŠµí•˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì´ í™•ë¥  ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ë³¸ì ì¸ ê³¼ì •ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì…¨ê¸°ë¥¼ ë°”ëë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "sTKWiPxXObI_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k36NRbTcOcUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial NB ì ìš©ì˜ ì˜ˆ. ( sklearnì˜ 20 newsgroup )"
      ],
      "metadata": {
        "id": "ran0i9CiVX9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Google Colabì—ì„œ matplotlib ì• ë‹ˆë©”ì´ì…˜ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ì„¤ì •\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# í•œê¸€ í‘œì‹œë¥¼ ìœ„í•œ ì„¤ì •\n",
        "!pip install koreanize-matplotlib\n",
        "import koreanize_matplotlib\n",
        "\n",
        "\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']  # ì¼ë¶€ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒ\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. íŒŒì´í”„ë¼ì¸ êµ¬ì„± (ë²¡í„°í™” + Naive Bayes)\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),         # ë‹¨ì–´ ê°œìˆ˜ ê¸°ë°˜ ë²¡í„°í™”\n",
        "    ('tfidf', TfidfTransformer()),       # TF-IDF ë³€í™˜\n",
        "    ('clf', MultinomialNB()),            # Naive Bayes ë¶„ë¥˜ê¸°\n",
        "])\n",
        "\n",
        "# 3. ëª¨ë¸ í•™ìŠµ\n",
        "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "# 4. ì˜ˆì¸¡ ë° í‰ê°€\n",
        "predicted = text_clf.predict(newsgroups_test.data)\n",
        "accuracy = metrics.accuracy_score(newsgroups_test.target, predicted)\n",
        "print(f\"ì •í™•ë„: {accuracy:.4f}\")\n",
        "\n",
        "# 5. ë¶„ë¥˜ ë³´ê³ ì„œ ì¶œë ¥\n",
        "print(metrics.classification_report(newsgroups_test.target, predicted, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# 6. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "conf_matrix = metrics.confusion_matrix(newsgroups_test.target, predicted)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(conf_matrix, annot=True, xticklabels=newsgroups_test.target_names, yticklabels=newsgroups_test.target_names, fmt='d', cmap='Blues')\n",
        "plt.xlabel('ì˜ˆì¸¡ ë ˆì´ë¸”')\n",
        "plt.ylabel('ì‹¤ì œ ë ˆì´ë¸”')\n",
        "plt.title('í˜¼ë™ í–‰ë ¬')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SD2RugX0Vff2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLPì—ì„œ ìì£¼ í™œìš©ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ ì„ ì‹¤ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "ë‹¨ì–´ ë²¡í„°í™” (TF-IDF, CountVectorizer ë“±)\n",
        "\n",
        "ì°¨ì› ì¶•ì†Œ (PCA, TruncatedSVD)\n",
        "\n",
        "í´ëŸ¬ìŠ¤í„°ë§ (KMeans ë“±)\n",
        "\n",
        "ë¶„ë¥˜ê¸° (Naive Bayes, SVM, Logistic Regression ë“±)"
      ],
      "metadata": {
        "id": "-4HzXSrTX5cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ” ì£¼ìš” ì„¤ëª…:\n",
        "CountVectorizer: ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜\n",
        "\n",
        "TfidfTransformer: ë‹¨ìˆœí•œ ì¹´ìš´íŠ¸ ëŒ€ì‹  TF-IDFë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
        "\n",
        "MultinomialNB: ë‹¤í•­ ë¶„í¬ ê¸°ë°˜ì˜ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ (í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ì í•©)\n",
        "\n",
        "Pipeline: ì „ì²˜ë¦¬ + ëª¨ë¸ì„ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ë¬¶ì–´ì„œ ì‚¬ìš©\n",
        "\n",
        "í˜¼ë™ í–‰ë ¬: ë¶„ë¥˜ ì„±ëŠ¥ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "\n",
        "TF-IDFì˜ full nameì€:\n",
        "\n",
        "Term Frequency â€“ Inverse Document Frequency\n",
        "\n",
        "ê° êµ¬ì„± ìš”ì†Œ ì„¤ëª…:\n",
        "Term Frequency (TF)\n",
        "\n",
        "íŠ¹ì • ë¬¸ì„œì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "ì˜ˆì‹œ: ë¬¸ì„œ Aì—ì„œ \"space\"ê°€ 3ë²ˆ ë‚˜ì™”ë‹¤ë©´ â†’ TF = 3\n",
        "\n",
        "Inverse Document Frequency (IDF)\n",
        "\n",
        "ì „ì²´ ë¬¸ì„œ ì§‘í•©ì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ í¬ê·€í•œì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´(ì˜ˆ: \"the\", \"is\")ì—ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜ë¥¼,\n",
        "ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´(ì˜ˆ: \"galaxy\")ì—ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "IDF\n",
        "(\n",
        "ğ‘¡\n",
        ")\n",
        "=\n",
        "log\n",
        "â¡\n",
        "(\n",
        "ğ‘\n",
        "1\n",
        "+\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        ")\n",
        "IDF(t)=log(\n",
        "1+n\n",
        "t\n",
        "â€‹\n",
        "\n",
        "N\n",
        "â€‹\n",
        " )\n",
        "ğ‘\n",
        "N: ì „ì²´ ë¬¸ì„œ ìˆ˜\n",
        "\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        "n\n",
        "t\n",
        "â€‹\n",
        " : ë‹¨ì–´\n",
        "ğ‘¡\n",
        "tê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜\n",
        "\n",
        "TF-IDF\n",
        "\n",
        "TF-IDF\n",
        "(\n",
        "ğ‘¡\n",
        ",\n",
        "ğ‘‘\n",
        ")\n",
        "=\n",
        "TF\n",
        "(\n",
        "ğ‘¡\n",
        ",\n",
        "ğ‘‘\n",
        ")\n",
        "Ã—\n",
        "IDF\n",
        "(\n",
        "ğ‘¡\n",
        ")\n",
        "TF-IDF(t,d)=TF(t,d)Ã—IDF(t)\n",
        "ë‹¨ì–´ê°€ íŠ¹ì • ë¬¸ì„œì— ë§ì´ ë‚˜ì˜¤ê³  ì „ì²´ì ìœ¼ë¡œëŠ” ë“œë¬¼ìˆ˜ë¡ í° ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ê°’ì€ ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ìˆ˜ì¹˜í™”í•  ìˆ˜ ìˆì–´, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ê²€ìƒ‰ì—”ì§„, ë¬¸ì„œ ìš”ì•½ ë“±ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "kGw_MEbxWHg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ì¶• í™•ì¸:\n",
        "\n",
        "ê°€ë¡œì¶• (ì˜ˆì¸¡ ë ˆì´ë¸”): ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 'alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc'ì˜ ë„¤ ê°€ì§€ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.\n",
        "ì„¸ë¡œì¶• (ì‹¤ì œ ë ˆì´ë¸”): ë°ì´í„°ì˜ ì‹¤ì œ í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì„¸ë¡œì¶•ì˜ ìˆœì„œëŠ” ê°€ë¡œì¶•ê³¼ ë™ì¼í•˜ê²Œ ë°°ì—´ë©ë‹ˆë‹¤.\n",
        "2. ê° ì…€ì˜ ì˜ë¯¸:\n",
        "\n",
        "ê° ì…€ì˜ ìˆ«ìëŠ” í•´ë‹¹ë˜ëŠ” ê²½ìš°ì˜ ë°ì´í„° ê°œìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "\n",
        "ëŒ€ê°ì„  ì…€ (True Positives & True Negatives):\n",
        "\n",
        "(1í–‰ 1ì—´, 238): ì‹¤ì œ í´ë˜ìŠ¤ê°€ 'alt.atheism'ì´ê³  ëª¨ë¸ë„ 'alt.atheism'ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (True Positive for 'alt.atheism')\n",
        "(2í–‰ 2ì—´, 343): ì‹¤ì œ í´ë˜ìŠ¤ê°€ 'comp.graphics'ì´ê³  ëª¨ë¸ë„ 'comp.graphics'ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (True Positive for 'comp.graphics')\n",
        "(3í–‰ 3ì—´, 364): ì‹¤ì œ í´ë˜ìŠ¤ê°€ 'sci.space'ì´ê³  ëª¨ë¸ë„ 'sci.space'ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (True Positive for 'sci.space')\n",
        "(4í–‰ 4ì—´, 18): ì‹¤ì œ í´ë˜ìŠ¤ê°€ 'talk.religion.misc'ì´ê³  ëª¨ë¸ë„ 'talk.religion.misc'ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (True Positive for 'talk.religion.misc')\n",
        "ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ê°ì„  ì…€ì˜ ê°’ì´ í´ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "ë¹„ëŒ€ê°ì„  ì…€ (False Positives & False Negatives):\n",
        "\n",
        "(1í–‰ 2ì—´, 8): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'alt.atheism'ì´ì§€ë§Œ ëª¨ë¸ì´ 'comp.graphics'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'alt.atheism', False Positive for 'comp.graphics')\n",
        "(1í–‰ 3ì—´, 68): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'alt.atheism'ì´ì§€ë§Œ ëª¨ë¸ì´ 'sci.space'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'alt.atheism', False Positive for 'sci.space')\n",
        "(1í–‰ 4ì—´, 5): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'alt.atheism'ì´ì§€ë§Œ ëª¨ë¸ì´ 'talk.religion.misc'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'alt.atheism', False Positive for 'talk.religion.misc')\n",
        "(2í–‰ 1ì—´, 8): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'comp.graphics'ì´ì§€ë§Œ ëª¨ë¸ì´ 'alt.atheism'ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'comp.graphics', False Positive for 'alt.atheism')\n",
        "(2í–‰ 3ì—´, 38): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'comp.graphics'ì´ì§€ë§Œ ëª¨ë¸ì´ 'sci.space'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'comp.graphics', False Positive for 'sci.space')\n",
        "(2í–‰ 4ì—´, 0): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'comp.graphics'ì´ì§€ë§Œ ëª¨ë¸ì´ 'talk.religion.misc'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'comp.graphics', False Positive for 'talk.religion.misc')\n",
        "(3í–‰ 1ì—´, 12): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'sci.space'ì´ì§€ë§Œ ëª¨ë¸ì´ 'alt.atheism'ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'sci.space', False Positive for 'alt.atheism')\n",
        "(3í–‰ 2ì—´, 18): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'sci.space'ì´ì§€ë§Œ ëª¨ë¸ì´ 'comp.graphics'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'sci.space', False Positive for 'comp.graphics')\n",
        "(3í–‰ 4ì—´, 0): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'sci.space'ì´ì§€ë§Œ ëª¨ë¸ì´ 'talk.religion.misc'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'sci.space', False Positive for 'talk.religion.misc')\n",
        "(4í–‰ 1ì—´, 174): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'talk.religion.misc'ì´ì§€ë§Œ ëª¨ë¸ì´ 'alt.atheism'ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'talk.religion.misc', False Positive for 'alt.atheism')\n",
        "(4í–‰ 2ì—´, 11): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'talk.religion.misc'ì´ì§€ë§Œ ëª¨ë¸ì´ 'comp.graphics'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'talk.religion.misc', False Positive for 'comp.graphics')\n",
        "(4í–‰ 3ì—´, 48): ì‹¤ì œ í´ë˜ìŠ¤ëŠ” 'talk.religion.misc'ì´ì§€ë§Œ ëª¨ë¸ì´ 'sci.space'ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. (False Negative for 'talk.religion.misc', False Positive for 'sci.space')\n",
        "ë¹„ëŒ€ê°ì„  ì…€ì˜ ê°’ì´ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ê³„ì‚° (í˜¼ë™ í–‰ë ¬ ê¸°ë°˜):"
      ],
      "metadata": {
        "id": "IJ_1bau_W0YK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-8kc2PhWIKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„ ê²°ê³¼ í•´ì„:\n",
        "\n",
        "ëª¨ë¸ì€ 'comp.graphics'ì™€ 'sci.space' í´ë˜ìŠ¤ë¥¼ ë¹„êµì  ì˜ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤ (ëŒ€ê°ì„  ê°’ì´ í¬ê³ , í•´ë‹¹ í–‰/ì—´ì˜ ì˜¤ë¶„ë¥˜ ê°’ì´ ì‘ìŒ).\n",
        "'alt.atheism' í´ë˜ìŠ¤ëŠ” 'sci.space'ë¡œ ì˜ëª» ë¶„ë¥˜ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤ (68ê°œ).\n",
        "'talk.religion.misc' í´ë˜ìŠ¤ì˜ ì¬í˜„ìœ¨ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (0.082). ì´ëŠ” ì‹¤ì œ 'talk.religion.misc' í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. íŠ¹íˆ 'alt.atheism'ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²½ìš°ê°€ 174ê°œë¡œ ë§¤ìš° ë§ìŠµë‹ˆë‹¤.\n",
        "5. ì¶”ê°€ ë¶„ì„ ë°©í–¥:\n",
        "\n",
        "í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸: ê° í´ë˜ìŠ¤ì˜ ì‹¤ì œ ë°ì´í„° ê°œìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ íŠ¹ì • í´ë˜ìŠ¤ì˜ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ëª¨ë¸ í•™ìŠµì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ì˜¤ë¶„ë¥˜ ì›ì¸ ë¶„ì„: íŠ¹ì • í´ë˜ìŠ¤ ê°„ì˜ ì˜¤ë¶„ë¥˜ê°€ ë§ì´ ë°œìƒí•˜ëŠ” ì´ìœ ë¥¼ ë°ì´í„°ì˜ íŠ¹ì„±ì´ë‚˜ ëª¨ë¸ì˜ í•œê³„ì ì—ì„œ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'talk.religion.misc'ì™€ 'alt.atheism'ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ìœ ì‚¬í•˜ì—¬ ëª¨ë¸ì´ í˜¼ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ëª¨ë¸ ê°œì„ : í˜¼ë™ í–‰ë ¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬, íŠ¹ì„± ì„ íƒ, ëª¨ë¸ íŠœë‹ ë“±ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "AGkqIMScWvOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
        "texts = [\n",
        "    \"spam spam spam spam love win winner prize\",\n",
        "    \"spam spam money offer limited deal\",\n",
        "    \"free money prize claim winner now\",\n",
        "    \"limited time offer win cash prize\",\n",
        "    \"you won a prize claim now\",  # spam\n",
        "\n",
        "    \"hello how are you doing today\",\n",
        "    \"let's meet for lunch and coffee\",\n",
        "    \"happy birthday have a great day\",\n",
        "    \"see you at the meeting tomorrow\",\n",
        "    \"thank you for your help\"  # ham\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1: spam, 0: ham\n",
        "\n",
        "# ë²¡í„°í™”\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = np.array(labels)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Multinomial Naive Bayes í›ˆë ¨\n",
        "clf = MultinomialNB()\n",
        "clf.partial_fit(X[:1], y[:1], classes=np.array([0, 1]))  # ì´ˆê¸°í™”\n",
        "\n",
        "# ì• ë‹ˆë©”ì´ì…˜ìš© ë°ì´í„° ì¤€ë¹„\n",
        "steps = []\n",
        "for i in range(2, len(X.toarray()) + 1):\n",
        "    clf.partial_fit(X[:i], y[:i])\n",
        "    steps.append(clf.feature_log_prob_.copy())\n",
        "\n",
        "# ì• ë‹ˆë©”ì´ì…˜ ìƒì„±\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bar_container = ax.bar(feature_names, np.exp(steps[0][1]), color='tomato')\n",
        "ax.set_ylim(0, 0.4)\n",
        "ax.set_title(\"Word Probabilities for Class 'Spam'\")\n",
        "ax.set_ylabel(\"Probability\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "def update(frame):\n",
        "    probs = np.exp(steps[frame][1])  # spam class\n",
        "    for bar, new_height in zip(bar_container, probs):\n",
        "        bar.set_height(new_height)\n",
        "    ax.set_title(f\"Word Probabilities for Class 'Spam' (Step {frame+2})\")\n",
        "    return bar_container\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(steps), interval=800, blit=False)\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "hTreNcHaWv-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}